{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:24: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:24: SyntaxWarning: invalid escape sequence '\\ '\n",
      "/var/folders/9k/l_xlgyyj413fr5n1ym7q20rc0000gn/T/ipykernel_25828/2627761421.py:24: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  \\       /  /|\\       / \\\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nBasics\\nEach value in the array is a weight that determines how much influence a particular input node has on a particular hidden layer node.\\nAfter transpose-:\\n toes  win %  fans\\n hid[0] 0.1   -0.1   0.1\\n hid[1] 0.2    0.1   0.4\\n hid[2] -0.1   0.9   0.1\\n\\n hidden to predict layer\\n hid[0]  hid[1]  hid[2]\\n hurt?  0.3     0.1     0.0\\n win?   1.1     0.2     1.3\\n sad?  -0.3     0.0     0.1\\n\\n So, hp_wgt[0, 1] = (0.1) is the weight connecting hid[1] to the hurt? prediction, \\n hp_wgt[2, 2] = (0.1) is the weight connecting hid[2] to the sad? prediction, and so on.\\n\\nHow it looks like-:\\n    Input Layer          Hidden Layer         Output Layer\\n    (3 nodes)            (3 nodes)           (3 nodes)\\n\\n      toes  --------> hid[0] --------> hurt?\\n             \\\\       /  /|\\\\       /       win % -----> hid[1] ---  ---  win?\\n             /     \\\\   |       \\\\ /\\n      fans --------> hid[2] --------> sad?\\n\\n      Weights:     ih_wgt          hp_wgt\\n\\nEverything we’ve done in this chapter is a form of what’s called forward propagation, wherein\\na neural network takes input data and makes a prediction. It’s called this because you’re\\npropagating activations forward through the network. In these examples, activations are all the\\nnumbers that are not weights and are unique for every prediction.\\nThe weights are static for a given trained network, while the activations change with each new input.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Basics\n",
    "Each value in the array is a weight that determines how much influence a particular input node has on a particular hidden layer node.\n",
    "After transpose-:\n",
    " toes  win %  fans\n",
    " hid[0] 0.1   -0.1   0.1\n",
    " hid[1] 0.2    0.1   0.4\n",
    " hid[2] -0.1   0.9   0.1\n",
    "\n",
    " hidden to predict layer\n",
    " hid[0]  hid[1]  hid[2]\n",
    " hurt?  0.3     0.1     0.0\n",
    " win?   1.1     0.2     1.3\n",
    " sad?  -0.3     0.0     0.1\n",
    "\n",
    " So, hp_wgt[0, 1] = (0.1) is the weight connecting hid[1] to the hurt? prediction, \n",
    " hp_wgt[2, 2] = (0.1) is the weight connecting hid[2] to the sad? prediction, and so on.\n",
    "\n",
    "How it looks like-:\n",
    "    Input Layer          Hidden Layer         Output Layer\n",
    "    (3 nodes)            (3 nodes)           (3 nodes)\n",
    "\n",
    "      toes  --------> hid[0] --------> hurt?\n",
    "             \\       /  /|\\       / \\\n",
    "      win % -----> hid[1] ---  ---  win?\n",
    "             /     \\   |       \\ /\n",
    "      fans --------> hid[2] --------> sad?\n",
    "\n",
    "      Weights:     ih_wgt          hp_wgt\n",
    "\n",
    "Everything we’ve done in this chapter is a form of what’s called forward propagation, wherein\n",
    "a neural network takes input data and makes a prediction. It’s called this because you’re\n",
    "propagating activations forward through the network. In these examples, activations are all the\n",
    "numbers that are not weights and are unique for every prediction.\n",
    "The weights are static for a given trained network, while the activations change with each new input.\n",
    "\n",
    "Data is presented like this\n",
    "ih_wgt = np.array(\n",
    "    [    toes  wl   fans\n",
    "        [0.1, 0.2, -0.1],  # hid[0]\n",
    "        [-0.1, 0.1, 0.9],  # hid[1]\n",
    "        [0.1, 0.4, 0.1],  # hid[2]\n",
    "    ]\n",
    ").T\n",
    "\n",
    "Transpose why?\n",
    "This is our input to hidden weight layer -> we need to multiply this with input (num_toes, wlrec, n_fans)\n",
    "So it should multiply like 0.1 * num_toes + wlrec * -0.1 + n_fans * 0.1 hence we transpose the matrix into this-:\n",
    "hid[0]  hid[1]  hid[2]\n",
    "toes      0.1     0.2    -0.1\n",
    "win%     -0.1     0.1     0.9\n",
    "fans      0.1     0.4     0.1\n",
    "\n",
    "A weight of 0.9 from 'win%' to 'hid[2]' means that the win/loss record has a strong positive influence on the activation of the third hidden node\n",
    "A weight of -0.1 from 'toes' to 'hid[2]' means that the number of toes has a small negative influence on that same hidden node\n",
    "The hidden layer is learning to detect different patterns or combinations of the input features.\n",
    "\n",
    "The output vector represents the network's prediction scores for each sentiment:\n",
    "\n",
    "0.21 for \"hurt?\"\n",
    "0.145 for \"win?\"\n",
    "0.506 for \"sad?\"\n",
    "This means that for the input [8.5, 0.65, 1.2], the network predicts the highest score for \"sad?\" (0.506), \n",
    "    suggesting this is the most likely sentiment outcome based on the patterns it has learned.\n",
    "\n",
    "Forward Pass: Input → Hidden → Output\n",
    "    We take our input values\n",
    "    We multiply by weights to get hidden layer activations\n",
    "    We multiply by more weights to get output predictions\n",
    "\n",
    "The Backward Pass (Learning)\n",
    "    Compare output to actual targets (what should have been predicted)\n",
    "    Calculate error/loss\n",
    "    Propagate error backward through the network\n",
    "    Update weights to reduce error\n",
    "    Repeat many times with training data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9800000000000001\n"
     ]
    }
   ],
   "source": [
    "# neural network\n",
    "import numpy as np\n",
    "\n",
    "weights = np.array([0.1, 0.2, 0])\n",
    "\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    pred = input.dot(weights)\n",
    "    return pred\n",
    "\n",
    "\n",
    "toes = np.array([8.5, 9.5, 9.9, 9.0])\n",
    "wlrec = np.array([0.65, 0.8, 0.8, 0.9])\n",
    "nfans = np.array([1.2, 1.3, 0.5, 1.0])\n",
    "input = np.array([toes[0], wlrec[0], nfans[0]])\n",
    "pred = neural_network(input, weights)\n",
    "print(pred)\n",
    "\n",
    "\n",
    "# dot product does the same as this\n",
    "def w_sum(a, b):\n",
    "    assert len(a) == len(b)\n",
    "    output = 0\n",
    "    for i in range(len(a)):\n",
    "        output += a[i] * b[i] # weights * input\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2135 0.145  0.5065]\n"
     ]
    }
   ],
   "source": [
    "# we use this neural network to feed output of this into another neural network\n",
    "# since finding patterns is hard, often a single weight matrix mul is not sufficient.\n",
    "# input -> hid(pred1) -> pred(final)\n",
    "import numpy as np\n",
    "\n",
    "        # toes % win # fans\n",
    "ih_wgt = np.array(\n",
    "    [\n",
    "        [0.1, 0.2, -0.1],  # hid[0]\n",
    "        [-0.1, 0.1, 0.9],  # hid[1]\n",
    "        [0.1, 0.4, 0.1],  # hid[2]\n",
    "    ]\n",
    ").T\n",
    "\n",
    "        # hid[0] hid[1] hid[2]\n",
    "hp_wgt = np.array(\n",
    "    [\n",
    "        [0.3, 1.1, -0.3],  # hurt?\n",
    "        [0.1, 0.2, 0.0],  # win?\n",
    "        [0.0, 1.3, 0.1],  # sad?\n",
    "    ]\n",
    ").T\n",
    "\n",
    "weights = [ih_wgt, hp_wgt]\n",
    "\n",
    "def neural_network(input, weights):\n",
    "    hid = input.dot(weights[0])\n",
    "    pred = hid.dot(weights[1])\n",
    "    return pred\n",
    "\n",
    "\n",
    "toes = np.array([8.5, 9.5, 9.9, 9.0])\n",
    "wlrec = np.array([0.65, 0.8, 0.8, 0.9])\n",
    "nfans = np.array([1.2, 1.3, 0.5, 1.0])\n",
    "input = np.array([toes[0], wlrec[0], nfans[0]])\n",
    "pred = neural_network(input, weights)\n",
    "# prediction of what factors num toes, win/loss rec, fans play on actual winning of match\n",
    "print(pred) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss : 0.10274602160572524\n",
      "Epoch 1000, Loss : 0.017924054333226427\n",
      "Epoch 2000, Loss : 0.016793841822455455\n",
      "Epoch 3000, Loss : 0.014440416455399905\n",
      "Epoch 4000, Loss : 0.012387851717950227\n",
      "Epoch 5000, Loss : 0.011838216443075502\n",
      "Epoch 6000, Loss : 0.011668971906495176\n",
      "Epoch 7000, Loss : 0.011589497419769088\n",
      "Epoch 8000, Loss : 0.011540781765255272\n",
      "Epoch 9000, Loss : 0.011506275597289613\n",
      "\n",
      "Game 1:\n",
      "Input: Toes=8.5, Win/Loss=0.65, Fans=1.2\n",
      "Prediction: Hurt=0.044, Win=0.890, Sad=0.194\n",
      "Target: Hurt=0.100, Win=0.900, Sad=0.200\n",
      "\n",
      "Game 2:\n",
      "Input: Toes=9.5, Win/Loss=0.8, Fans=1.3\n",
      "Prediction: Hurt=0.054, Win=0.807, Sad=0.304\n",
      "Target: Hurt=0.000, Win=0.800, Sad=0.300\n",
      "\n",
      "Game 3:\n",
      "Input: Toes=9.9, Win/Loss=0.8, Fans=0.5\n",
      "Prediction: Hurt=0.499, Win=0.502, Sad=0.499\n",
      "Target: Hurt=0.700, Win=0.200, Sad=0.500\n",
      "\n",
      "Game 4:\n",
      "Input: Toes=9.0, Win/Loss=0.9, Fans=1.0\n",
      "Prediction: Hurt=0.302, Win=0.898, Sad=0.105\n",
      "Target: Hurt=0.300, Win=0.900, Sad=0.100\n",
      "\n",
      "New prediction for Toes=8.8, Win/Loss=0.12, Fans=0.0:\n",
      "Hurt=0.274, Win=0.482, Sad=0.585\n"
     ]
    }
   ],
   "source": [
    "# Weight Updates:\n",
    "\n",
    "# For hidden→prediction weights: Create a gradient using the hidden layer activations and output deltas\n",
    "# For input→hidden weights: Create a gradient using the input values and hidden deltas\n",
    "# Subtract a portion (learning_rate) of these gradients from the weights\n",
    "\n",
    "# Building the entire network\n",
    "ih_wgt = np.random.randn(3, 3) * 0.1 \n",
    "hp_wgt = np.random.randn(3, 3) * 0.1\n",
    "weights = [ih_wgt, hp_wgt]\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# forward\n",
    "def neuralnet(input, weights):\n",
    "    hid_inp = input.dot(weights[0])\n",
    "    hid = sigmoid(hid_inp)\n",
    "    pred_inp = hid.dot(weights[1])\n",
    "    pred = sigmoid(pred_inp)\n",
    "    return pred, hid, hid_inp, pred_inp\n",
    "\n",
    "toes = np.array([8.5, 9.5, 9.9, 9.0])\n",
    "wlrec = np.array([0.65, 0.8, 0.8, 0.9])\n",
    "nfans = np.array([1.2, 1.3, 0.5, 1.0])\n",
    "\n",
    "# normalise\n",
    "toes_norm = (toes - np.mean(toes)) / np.std(toes)\n",
    "wlrec_norm = (wlrec - np.mean(wlrec)) / np.std(wlrec)\n",
    "nfans_norm = (nfans - np.mean(nfans)) / np.std(nfans)\n",
    "\n",
    "# Feature Matrix\n",
    "X = np.column_stack((toes_norm, wlrec_norm, nfans_norm))\n",
    "\n",
    "# Target values (example: these would be the actual sentiments for each game)\n",
    "# Format: [hurt?, win?, sad?]\n",
    "y = np.array([\n",
    "    [0.1, 0.9, 0.2],  # Game 1: Likely win, not hurt, not sad\n",
    "    [0.0, 0.8, 0.3],  # Game 2: Likely win, not hurt, slightly sad\n",
    "    [0.7, 0.2, 0.5],  # Game 3: Likely hurt, not win, somewhat sad\n",
    "    [0.3, 0.9, 0.1]   # Game 4: Likely win, slightly hurt, not sad\n",
    "])\n",
    "\n",
    "lr, epochs = 0.1, 10000\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for i in range(len(X)):\n",
    "        input_data = X[i]\n",
    "        target = y[i]\n",
    "        pred, hid, hid_inp, pred_inp = neuralnet(input_data, weights)\n",
    "        loss = np.mean((pred - target) ** 2)\n",
    "        total_loss += loss\n",
    "\n",
    "        # backpropagation\n",
    "        err = pred - target\n",
    "        err_delta = err * sigmoid_derivative(pred) \n",
    "        # hidden_to_pred layer error\n",
    "        hidden_err = err_delta.dot(weights[1].T)\n",
    "        hidden_delta = hidden_err * sigmoid_derivative(hid)\n",
    "        # update weights\n",
    "        # basically gradient here is the derivative we do in gradient descent. but how do we calculate this in terms of matrix?\n",
    "        # wasnt it this? gradients = 1 / m * (X.T @ error)\n",
    "        # If I turn the dial between hid[i] and output[j], how much will the error change?\n",
    "        hp_pred_gradient = np.outer(hid, err_delta)\n",
    "        weights[1] -= lr * hp_pred_gradient\n",
    "        # input-> hidden\n",
    "        ih_pred_gradient = np.outer(input_data, hidden_delta) # input * hidden me kitna error tha (what does hidden_delta mean here)\n",
    "        weights[0] -= lr * ih_pred_gradient\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss : {total_loss/len(X)}\")\n",
    "\n",
    "# Test trained network\n",
    "for i in range(len(X)):\n",
    "    pred, _, _, _ = neuralnet(X[i], weights)\n",
    "    print(f\"\\nGame {i+1}:\")\n",
    "    print(f\"Input: Toes={toes[i]}, Win/Loss={wlrec[i]}, Fans={nfans[i]}\")\n",
    "    print(f\"Prediction: Hurt={pred[0]:.3f}, Win={pred[1]:.3f}, Sad={pred[2]:.3f}\")\n",
    "    print(f\"Target: Hurt={y[i,0]:.3f}, Win={y[i,1]:.3f}, Sad={y[i,2]:.3f}\")\n",
    "\n",
    "\n",
    "# Make a new prediction (new game data)\n",
    "new_input = np.array([8.8, 0.12, 0])  # (num_toes, wl, fans) ~ not going to win, going to be pretty sad (actual ans : Hurt=0.139, Win=0.433, Sad=0.684 Checks out)\n",
    "new_input_norm = (new_input - np.array([np.mean(toes), np.mean(wlrec), np.mean(nfans)])) / np.array([np.std(toes), np.std(wlrec), np.std(nfans)])\n",
    "new_pred, _, _, _ = neuralnet(new_input_norm, weights)\n",
    "print(f\"\\nNew prediction for Toes={new_input[0]}, Win/Loss={new_input[1]}, Fans={new_input[2]}:\")\n",
    "print(f\"Hurt={new_pred[0]:.3f}, Win={new_pred[1]:.3f}, Sad={new_pred[2]:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nNumPy version (Regression-like output): \\nproduced three separate output values, which you interpreted as raw probabilities for \"hurt\", \"win\", and \"sad\" independently. \\nFor example, [0.053, 0.887, 0.190]. This is more like a multi-output regression or predicting independent probabilities.\\n\\nPyTorch version (Multi-class Classification): \\nThis version predicts one single class out of the three possibilities. \\nIt assumes that for any given input, only one of \"Hurt\", \"Win\", or \"Sad\" is the primary sentiment. \\nThe output is a single class label (0, 1, or 2).\\n\\nPredicted classes: tensor([1, 1, 0, 2]) means:\\n\\n\\nFor the 1st input sample, the network predicts Class 1 (\"Win\").\\nFor the 2nd input sample, the network predicts Class 1 (\"Win\").\\nFor the 3rd input sample, the network predicts Class 0 (\"Hurt\").\\nFor the 4th input sample, the network predicts Class 2 (\"Sad\").\\n\\nNumPy Version (Predicting individual sentiment scores/probabilities):\\n\\nQuestion it answers: \"For this game, what is the probability of being hurt? What is the probability of winning? What is the probability of being sad?\" (These could be independent or sum to 1, depending on how you train it).\\nTarget format: A vector of scores/probabilities for each sentiment, e.g., [0.1, 0.9, 0.2].\\nAnalogy: Predicting the individual scores a student gets in Math, Science, and English.\\n\\n\\n\\nPyTorch Version (Predicting a single dominant sentiment class):\\n\\nQuestion it answers: \"For this game, what is the single most dominant sentiment: Hurt, Win, or Sad?\"\\nTarget format: A single integer representing the class label, e.g., 1 (for \"Win\").\\nAnalogy: Predicting a student\\'s overall grade category (A, B, C) based on their performance.\\n\\nIn your NumPy code, y provided a vector of target scores for each sample.\\nIn your PyTorch code, targets provides a single class label for each sample, chosen from 3 possible classes.\\n\\nHow inp / features looks like-:\\n       toes  wlrec  nfans\\n      --------------------\\nSample 1: [8.5,  0.65,  1.2 ]\\nSample 2: [9.5,  0.8,   1.3 ]\\nSample 3: [9.9,  0.8,   0.5 ]\\nSample 4: [9.0,  0.9,   1.0 ]\\n\\ntargets: class indices (0=Hurt, 1=Win, 2=Sad) : Predicted classes: tensor([1, 1, 0, 2])\\nSample 1 : Win, Sample 2 : Win, Sample 3 : Hurt, Sample 4 : Sad {can verify}\\n'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "NumPy version (Regression-like output): \n",
    "produced three separate output values, which you interpreted as raw probabilities for \"hurt\", \"win\", and \"sad\" independently. \n",
    "For example, [0.053, 0.887, 0.190]. This is more like a multi-output regression or predicting independent probabilities.\n",
    "\n",
    "PyTorch version (Multi-class Classification): \n",
    "This version predicts one single class out of the three possibilities. \n",
    "It assumes that for any given input, only one of \"Hurt\", \"Win\", or \"Sad\" is the primary sentiment. \n",
    "The output is a single class label (0, 1, or 2).\n",
    "\n",
    "Predicted classes: tensor([1, 1, 0, 2]) means:\n",
    "\n",
    "\n",
    "For the 1st input sample, the network predicts Class 1 (\"Win\").\n",
    "For the 2nd input sample, the network predicts Class 1 (\"Win\").\n",
    "For the 3rd input sample, the network predicts Class 0 (\"Hurt\").\n",
    "For the 4th input sample, the network predicts Class 2 (\"Sad\").\n",
    "\n",
    "NumPy Version (Predicting individual sentiment scores/probabilities):\n",
    "\n",
    "Question it answers: \"For this game, what is the probability of being hurt? What is the probability of winning? What is the probability of being sad?\" (These could be independent or sum to 1, depending on how you train it).\n",
    "Target format: A vector of scores/probabilities for each sentiment, e.g., [0.1, 0.9, 0.2].\n",
    "Analogy: Predicting the individual scores a student gets in Math, Science, and English.\n",
    "\n",
    "\n",
    "\n",
    "PyTorch Version (Predicting a single dominant sentiment class):\n",
    "\n",
    "Question it answers: \"For this game, what is the single most dominant sentiment: Hurt, Win, or Sad?\"\n",
    "Target format: A single integer representing the class label, e.g., 1 (for \"Win\").\n",
    "Analogy: Predicting a student's overall grade category (A, B, C) based on their performance.\n",
    "\n",
    "In your NumPy code, y provided a vector of target scores for each sample.\n",
    "In your PyTorch code, targets provides a single class label for each sample, chosen from 3 possible classes.\n",
    "\n",
    "How inp / features looks like-:\n",
    "       toes  wlrec  nfans\n",
    "      --------------------\n",
    "Sample 1: [8.5,  0.65,  1.2 ]\n",
    "Sample 2: [9.5,  0.8,   1.3 ]\n",
    "Sample 3: [9.9,  0.8,   0.5 ]\n",
    "Sample 4: [9.0,  0.9,   1.0 ]\n",
    "\n",
    "targets: class indices (0=Hurt, 1=Win, 2=Sad) : Predicted classes: tensor([1, 1, 0, 2])\n",
    "Sample 1 : Win, Sample 2 : Win, Sample 3 : Hurt, Sample 4 : Sad {can verify}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss : 1.1407\n",
      "Epoch 500, Loss : 0.0006\n",
      "Epoch 1000, Loss : 0.0002\n",
      "Epoch 1500, Loss : 0.0001\n",
      "\n",
      "Predicted class probabilities:\n",
      " tensor([[1.7477e-05, 9.9998e-01, 3.7920e-06],\n",
      "        [1.1746e-05, 9.9997e-01, 1.3212e-05],\n",
      "        [9.9997e-01, 5.3250e-06, 2.0332e-05],\n",
      "        [2.4065e-06, 4.0955e-05, 9.9996e-01]])\n",
      "Predicted classes: tensor([1, 1, 0, 2])\n",
      "True classes: tensor([1, 1, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "# same in pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "toes = torch.tensor([8.5, 9.5, 9.9, 9.0])\n",
    "wlrec = torch.tensor([0.65, 0.8, 0.8, 0.9])\n",
    "nfans = torch.tensor([1.2, 1.3, 0.5, 1.0])\n",
    "\n",
    "# Normalize features\n",
    "features = torch.stack([toes, wlrec, nfans], dim=1)\n",
    "features = (features - features.mean(dim=0)) / features.std(dim=0)\n",
    "# targets: class indices (0=Hurt, 1=Win, 2=Sad)\n",
    "targets = torch.tensor([1, 1, 0, 2])  \n",
    "# if you want to it like numpy -> make targets normal not classes and use sigmoid instead of crossentropy and loss = nn.MSELoss()\n",
    "# targets_multilabel = torch.tensor([\n",
    "#     [0.1, 0.9, 0.2],  # Game 1\n",
    "#     [0.0, 0.8, 0.3],  # Game 2\n",
    "#     [0.7, 0.2, 0.5],  # Game 3\n",
    "#     [0.3, 0.9, 0.1]   # Game 4\n",
    "# ], dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(features, targets)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "class PyTorchNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inp_to_hidden = nn.Linear(3, 3)\n",
    "        self.relu = nn.ReLU() # activation function\n",
    "        self.hidden_to_pred = nn.Linear(3, 3)\n",
    "\n",
    "    # Takes input\n",
    "    def forward(self, x):\n",
    "        x = self.inp_to_hidden(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.hidden_to_pred(x)\n",
    "        return x \n",
    "model = PyTorchNet()\n",
    "loss = nn.CrossEntropyLoss() # target - pred\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01) # weights -= learning_rate * gradient\n",
    "epochs = 2000\n",
    "for epoch in range(epochs):\n",
    "    for xb, yb in loader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(xb)\n",
    "        final_loss = loss(pred, yb)\n",
    "        final_loss.backward()\n",
    "        optimizer.step() \n",
    "    if epoch % 500 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss : {final_loss.item():.4f}\")\n",
    "\n",
    "# prediction\n",
    "with torch.no_grad():\n",
    "    pred = model(features)\n",
    "    probs = torch.softmax(pred, dim=1)\n",
    "    prediction = torch.argmax(probs, dim=1)\n",
    "    print(\"\\nPredicted class probabilities:\\n\", probs)\n",
    "    print(\"Predicted classes:\", prediction)\n",
    "    print(\"True classes:\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
